# ðŸ¤– Ollama Model Comparator

A simple web application that allows you to compare responses from different Ollama models side by side.

## Features

- Compare multiple Ollama models on the same prompt
- Visualize differences between model outputs
- Measure performance metrics like response time and token generation speed
- Analyze similarity between different model responses
- Customize parameters like temperature and max tokens
- Support for custom models not in your local Ollama instance

## Installation

1. Make sure you have [Ollama](https://ollama.ai/) installed and running
2. Install the required Python packages:

```bash
pip install gradio requests
```

3. Run the application:

```bash
python app.py
```

4. Open your web browser to http://127.0.0.1:7860

## Usage

1. Enter your Ollama API URL (default is http://localhost:11434)
2. Select models from the dropdown or add custom model names
3. Enter your prompt (and optional system prompt)
4. Adjust temperature and max tokens if needed
5. Click "Generate Responses"
6. View and compare the results in the tabs

## Why Use This?

- **Model Evaluation**: Compare different models to find the best one for your use case
- **Parameter Tuning**: Test how temperature affects different models
- **Benchmarking**: Compare performance metrics across models
- **Bias Testing**: See how different models respond to the same prompts
- **Learning**: Understand the strengths and weaknesses of various models

## License

MIT