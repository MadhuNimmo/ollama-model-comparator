# ü§ñ Ollama Model Comparator

![Ollama Model Comparator](assets/logo.png)

A powerful web application that allows you to compare responses from different Ollama models side by side. Perfect for evaluating, comparing, and analyzing the performance and output quality of various LLMs.

![GitHub License](https://img.shields.io/github/license/yourusername/ollama-model-comparator)
![Python Version](https://img.shields.io/badge/python-3.7%2B-blue)

## ‚ú® Features

- **Multi-model comparison**: Test the same prompt against multiple Ollama models simultaneously
- **Side-by-side comparison**: View all model responses in a tabbed interface
- **Diff view**: See exactly how responses differ between any two models with highlighted differences
- **Performance metrics**: Compare response time and tokens per second
- **Similarity analysis**: View a matrix showing how similar each model's responses are to others
- **Customizable settings**: Adjust temperature and max token settings

## üì∑ Screenshots

![App Interface](assets/screenshots/app_interface.png)

## üöÄ Getting Started

### Prerequisites

- Python 3.7+
- [Ollama](https://ollama.ai/) installed and running locally (or on a remote server)

### Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/ollama-model-comparator.git
cd ollama-model-comparator
```

2. Install the required dependencies:

```bash
pip install -r requirements.txt
```

3. Make sure Ollama is running on your system. The default Ollama endpoint is `http://localhost:11434`.

4. Download some models in Ollama (if you haven't already):

```bash
ollama pull llama3 mistral tiny-llama codellama
```

## üîç Usage

### Running the Application

1. Start the application:

```bash
python app.py
```

2. Open your web browser and go to `http://127.0.0.1:7860` to access the interface.

### Command Line Options

```
usage: app.py [-h] [--host HOST] [--port PORT] [--ollama-url OLLAMA_URL] [--share] [--debug]

Ollama Model Comparator - Compare different LLMs side by side

options:
  -h, --help            show this help message and exit
  --host HOST           Host to run the Gradio interface on (default: localhost)
  --port PORT           Port to run the Gradio interface on (default: 7860)
  --ollama-url OLLAMA_URL
                        Ollama API URL (default: http://localhost:11434)
  --share               Create a shareable link for the interface
  --debug               Enable debug mode
```

### Using the Interface

1. **Select models**: Choose which models you want to compare (at least one, preferably two or more)
2. **Enter a prompt**: Type your prompt in the input box
3. **Optional**: Set a system prompt, adjust temperature and max tokens
4. **Generate**: Click the "Generate Responses" button to see the results
5. **Compare**: View the different tabs to see each model's response, or use the "Side-by-Side Diff" view
6. **Analyze**: Examine the similarity matrix and performance statistics

## üõ†Ô∏è Advanced Configuration

### Connecting to a Remote Ollama Instance

To connect to a remote Ollama instance, use the `--ollama-url` parameter:

```bash
python app.py --ollama-url "http://your-server-ip:11434"
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üîÆ Future Plans

- [ ] Add support for streaming responses
- [ ] Implement custom prompt templates
- [ ] Add visualization options for comparison metrics
- [ ] Include chat history for conversational comparisons
- [ ] Add export functionality for reports
- [ ] Implement user authentication for shared instances

## üôè Acknowledgements

- [Ollama](https://ollama.ai/) for making local LLM deployment easy
- [Gradio](https://www.gradio.app/) for the web interface framework
