# ðŸ¤– Ollama Model Comparator

A simple tool to compare responses from different Ollama models side by side in real-time.

![Ollama Model Comparator](screenshot.png)

## Features

- **Side-by-side comparison**: See responses from two models simultaneously
- **Real-time streaming**: Responses appear as they're generated
- **Live statistics**: Track generation time and token count
- **Easy model selection**: Choose from installed models or add custom ones
- **Adjustable parameters**: Control temperature, max tokens, and timeout

## Installation

1. Make sure you have [Ollama](https://ollama.ai/) installed and running
2. Install Python dependencies:

```bash
pip install gradio requests
```

3. Run the application:

```bash
python app.py
```

4. Open your browser to http://127.0.0.1:7860

## Usage

1. Select two models from the dropdown menus
2. If your models aren't listed, add them in the "Add Custom Models" field
3. Enter your prompt (and optional system prompt)
4. Adjust temperature, max tokens, and timeout if needed
5. Click "Compare Models"
6. Watch the responses generate in real-time side by side

## Notes

- Models must be installed in Ollama before comparison
- Use the "Refresh Model List" button if you've added new models
- If you encounter timeout errors with large models, increase the timeout setting
- Responses are streamed in real-time as they're generated

## Why Use This Tool?

- **Model Evaluation**: Compare different models to find the best one for your use case
- **Parameter Tuning**: Test how temperature affects different models
- **Learning**: Understand the strengths and weaknesses of various models
- **Response Analysis**: Compare model behaviors on specific prompts

## License

MIT